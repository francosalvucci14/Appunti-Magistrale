# -*- coding: utf-8 -*-
"""ML_Lab7_2025_2026_Decision Tree

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n2akKFUYmXYRCYnkXCAQsaF3uWXivU7X

# ğŸ“˜ Machine Learning 2025/2026  

# Laboratorio 7 â€” ğŸŒ³ Decision Trees
### Docenti: Danilo Croce, Giorgio Gambosi  

In questo notebook costruiremo da zero una pipeline completa per **comprendere, addestrare e visualizzare** un *Decision Tree Classifier*.

Lâ€™obiettivo non Ã¨ solo â€œfarlo funzionareâ€, ma capire perchÃ© funziona e come prendere decisioni corrette nella pratica.

---

# ğŸ¯ Obiettivi del notebook

1. **Capire come funziona un Decision Tree**  
   â€“ struttura ad albero  
   â€“ nodi di split  
   â€“ impuritÃ   
   â€“ criteri di scelta dello split  

2. **Implementare il framework pratico del machine learning**  
   â€“ download di *Iris*  
   â€“ shuffle  
   â€“ split 80% / 10% / 10%

3. **Addestrare un Decision Tree con scikit-learn**

4. **Visualizzare e interpretare lâ€™albero**

5. **Comprendere gli indici di impuritÃ **  
   â€“ Gini  
   â€“ Entropia  
   â€“ Misclassification Error  
   â€“ perchÃ© questi criteri portano a scelte diverse

---

# ğŸŒ³ 1. Decision Trees: Idee di base

Un decision tree Ã¨ un modello predittivo che ripetutamente:

1. **divide i dati** in base ai valori di una feature  
2. **crea nodi** che rappresentano domande binarie (o multi-split)  
3. **raggiunge foglie** che contengono una predizione (classe o valore)

Esempio:  
> *â€œPetal length < 2.45?â€*  
> â€“ sÃ¬ â†’ classe *setosa*  
> â€“ no â†’ continua a dividere

Un albero Ã¨ quindi una sequenza di **decisioni gerarchiche**, con lo scopo di creare regioni dello spazio dei dati che siano **pure**, cioÃ¨ che contengano quasi solo una classe.

## ğŸŒ³ 1.1 Le tre famiglie di algoritmi: CART, ID3 e C4.5  
Prima di vedere il funzionamento degli split, Ã¨ utile conoscere rapidamente le tre principali famiglie storiche di algoritmi che costruiscono alberi decisionali.

### ğŸ”´ CART (Classification and Regression Trees)  
Proposto da Leo Breiman et al. nel 1984.  
Caratteristiche principali:  
- supporta **classificazione e regressione**  
- costruisce **solo split binari**  
- criterio di impuritÃ  tipico: **Gini** (classificazione) o **MSE** (regressione)  
- efficiente, stabile e alla base di molte implementazioni moderne  
Ãˆ lo standard pratico, ad es. in scikitâ€‘learn.

---

### ğŸ”µ ID3 (Iterative Dichotomiser 3)  
Sviluppato da Ross Quinlan nel 1986.  
Caratteristiche principali:  
- usa **entropia / information gain** come criterio di split  
- lavora bene con feature **categoriche**  
- originariamente non gestiva bene feature continue o valori mancanti  
- tende a costruire alberi profondi senza pruning automatico

---

### ğŸŸ¢ C4.5  
Evoluzione di ID3, uscita nel 1993 e sempre di Quinlan.  
Caratteristiche chiave:  
- gestisce anche **feature continue**, trovando soglie ottimali  
- introduce il **Gain Ratio**, che normalizza lâ€™information gain  
- supporta **valori mancanti**  
- incorpora una strategia di **pruning** per evitare lâ€™overfitting  
- produce facilmente regole â€œif-thenâ€ oltre allâ€™albero

---

### Riassunto cronologico e comparativo

| Algoritmo | Anno | Criterio | Tipo split | Caratteristiche principali |
|-----------|------|----------|------------|----------------------------|
| **CART**  | 1984 | Gini/MSE | binari      | classificazione + regressione, molto usato |
| **ID3**   | 1986 | Entropia | multi-valore| orientato a dati categorici, senza pruning |
| **C4.5**  | 1993 | Gain Ratio | multi-valore| gestisce continui, pruning, valori mancanti |

CosÃ¬ lâ€™evoluzione storica Ã¨ chiara: dallâ€™approccio generale di CART, alla formulazione di ID3 per classificazione pura con entropia, fino ai miglioramenti di C4.5 per rendere gli alberi piÃ¹ robusti e applicabili a scenari reali.

---

# ğŸ§  2. Algoritmo Greedy: ID3 / CART (versione concettuale)

I moderni alberi di classificazione (CART) funzionano cosÃ¬:


# Pseudocodice dellâ€™algoritmo CART per decision tree (classificazione)

Lâ€™algoritmo CART costruisce un albero di decisione scegliendo, a ogni nodo, la split che massimizza la purezza dei sottoinsiemi figli.

---

## ğŸ¯ Obiettivo
Costruire un albero binario che minimizzi un criterio di impuritÃ  (Gini, entropia o misclassification error).

---

## ğŸ”— Pseudocodice

**FUNCTION** `BuildTree(dataset D, depth)`  
1. **Se** D Ã¨ puro (tutti della stessa classe) **o** depth = max_depth **o** $|D| < \text{min\_samples\_split}$:  
  â€ƒâ€ƒ**return** `Leaf(label = majority_class(D))`

2. Per ogni feature $f$  
  â€ƒPer ogni possibile valore di split $s$  
  â€ƒâ€ƒ- Dividi D in due insiemi:  
  â€ƒâ€ƒâ€ƒ$D_{\text{left}} = \{ x \in D : x[f] \le s \}$  
  â€ƒâ€ƒâ€ƒ$D_{\text{right}} = \{ x \in D : x[f] > s \}$  
  â€ƒâ€ƒ- Calcola lâ€™impuritÃ  dei figli:  
  â€ƒâ€ƒâ€ƒ$I_{\text{split}} = \frac{|D_L|}{|D|} I(D_L) + \frac{|D_R|}{|D|} I(D_R)$  
  â€ƒâ€ƒ- Tieni traccia della split con **impuritÃ  minima**

3. Sia $(f^*, s^*)$ la migliore split trovata  
4. Crea nodo interno con split $(f^*, s^*)$

5. `left_child = BuildTree(D_left^*, depth+1)`  
6. `right_child = BuildTree(D_right^*, depth+1)`

7. **return** nodo con figli left/right



Caratteristiche importanti:
- Ã¨ un algoritmo **greedy**  
- non guarda lo split futuro, solo quello attuale  
- cerca lo split che **massimizza la purezza delle foglie**

---

# ğŸ“‰ 3. PerchÃ© servono misure di impuritÃ ?

Se ogni split servisse solo a "dividere", lâ€™albero sarebbe arbitrario.  
Abbiamo bisogno di una **funzione obiettivo** che misura quanto un nodo Ã¨ â€œpulitoâ€.

Una foglia Ã¨ perfettamente pura quando contiene campioni di una sola classe.

Gli indici standard che studieremo sono:

1. **Gini Impurity** (default in CART)
2. **Entropia** (usata in ID3 / C4.5)
3. **Classification Error** (piÃ¹ grezzo)

Discuteremo dopo le differenze teoriche e pratiche.

---

> Ora possiamo passare alla parte pratica:
> **download del dataset Iris, shuffle e split 80/10/10**.

# ğŸ“¥ 4. Caricamento del Dataset Iris, Shuffle e Split 80/10/10

Il dataset *Iris* Ã¨ uno dei dataset piÃ¹ utilizzati in machine learning:  
- 150 esempi  
- 4 feature continue  
- 3 classi bilanciate  
- nessun valore mancante  

Perfetto per studiare algoritmi e visualizzare modelli.

---

## ğŸ”„ 4.1 Caricamento del dataset

Useremo `sklearn.datasets.load_iris`, che restituisce:
- una matrice di feature `X âˆˆ â„^{150Ã—4}`
- un vettore target `y âˆˆ {0,1,2}`

---

## ğŸ”€ 4.2 Shuffle dei dati

Anche se il dataset Ã¨ giÃ  ben mescolato, **in ML non si dÃ  mai per scontato**:

- garantiamo che i dati siano in ordine casuale  
- evitiamo pattern non desiderati (ad esempio, classi raggruppate)

Il mescolamento deve essere **riproducibile**, quindi useremo un `random_state`.

---

## âœ‚ï¸ 4.3 Split 80% / 10% / 10%

Realizziamo la classica divisione:

- **80% train** â†’ per addestrare il modello  
- **10% validation** â†’ per scegliere iperparametri  
- **10% test** â†’ per valutazione finale  

Scikit-learn non offre direttamente uno split a tre vie, quindi lo faremo in due step:

1. split train+val vs test  
2. split train vs validation  

---

### ğŸ“Œ Risultati attesi dopo questo blocco

Dovremmo ottenere:

| Set | Dimensione (circa) | Uso |
|-----|---------------------|------|
| **Train** | 120 esempi | training del modello |
| **Validation** | 15 esempi | tuning di iperparametri / stopping |
| **Test** | 15 esempi | valutazione finale e onesta |

Una volta preparati i dati possiamo procedere allâ€™**addestramento del Decision Tree**.
"""

# ------------------------------------------------------------
# Decision Tree con scikit-learn
# ------------------------------------------------------------
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# 1) Caricamento e suddivisione del dataset
# ------------------------------------------------------------
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
class_names = iris.target_names

# shuffle + split 80 / 10 / 10
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.20, shuffle=True, random_state=23
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, shuffle=True, random_state=23
)

print("Train:", X_train.shape)
print("Val  :", X_val.shape)
print("Test :", X_test.shape)

# ------------------------------------------------------------
# 2) Addestramento del Decision Tree
# ------------------------------------------------------------
dt = DecisionTreeClassifier(
    criterion="entropy",        # alternative: "gini" (default), "log_loss"
    max_depth=None,          # nessun limite per ora (attenzione all'overfitting)
    min_samples_split=2,
    random_state=42
)

dt.fit(X_train, y_train)

# ------------------------------------------------------------
# 3) Valutazione su validation e test set
# ------------------------------------------------------------
val_pred = dt.predict(X_val)
test_pred = dt.predict(X_test)

acc_val = accuracy_score(y_val, val_pred)
acc_test = accuracy_score(y_test, test_pred)

print("\nAccuracy Validation:", acc_val)
print("Accuracy Test      :", acc_test)

# ------------------------------------------------------------
# 4) Visualizzazione dell'albero
# ------------------------------------------------------------
plt.figure(figsize=(14, 8))
plot_tree(
    dt,
    feature_names=feature_names,
    class_names=class_names,
    filled=True,
    rounded=True
)
plt.show()

"""# ğŸ“‰ 5. Indici di ImpuritÃ  nei Decision Trees

Quando un albero decide **qual Ã¨ la migliore split**, deve valutare quanto i nodi figli siano *puri*, cioÃ¨ quanto contengano una sola classe.

Per questo si usano **indici di impuritÃ **: funzioni che misurano quanto un insieme di esempi Ã¨ â€œmescolatoâ€.

Sia $p_k$ la proporzione di esempi della classe $k$ in un nodo.

---

## 1ï¸âƒ£ Misclassification Error

Lâ€™indice piÃ¹ semplice.

$$
\text{Error} = 1 - \max_k p_k
$$

- vale 0 quando una classe domina completamente  
- misura solo â€œquanto spesso sbaglierei predicendo la classe maggioritariaâ€

**Pro:** semplice da interpretare  
**Contro:** poco sensibile a cambiamenti nella distribuzione delle classi

---

## 2ï¸âƒ£ Gini Impurity (default in CART)

$$
\text{Gini} = \sum_k p_k (1 - p_k)
$$

Interpretazione intuitiva:  
probabilitÃ  che due esempi presi a caso appartengano a classi diverse.

**Pro:**  
- piÃ¹ sensibile di misclassification  
- semplice e veloce da calcolare  
- spesso simile allâ€™entropia nella pratica

**Contro:**  
- interpretazione piÃ¹ â€œmeccanicaâ€

---

## 3ï¸âƒ£ Entropia (criterio di ID3 / C4.5)

$$
\text{Entropy} = -\sum_k p_k \log p_k
$$

Deriva dalla teoria dellâ€™informazione: misura lâ€™incertezza nel nodo.

**Pro:**  
- ottima interpretazione teorica  
- molto sensibile alle variazioni di distribuzione

**Contro:**  
- piÃ¹ costosa da calcolare (logaritmi)

---

# ğŸ”¬ Differenze pratiche

Nella pratica:

- **Gini** e **Entropia** producono spesso alberi simili  
- **Gini** Ã¨ leggermente piÃ¹ veloce â†’ default in CART  
- **Entropia** Ã¨ piÃ¹ interpretabile  
- **Misclassification** Ã¨ troppo grezzo per costruire lâ€™albero  
  (ma utile nel pruning)

---

# ğŸ‘ Quale criterio scegliere?

| Obiettivo | Criterio consigliato |
|----------|-----------------------|
| VelocitÃ  | Gini |
| Interpretazione teorica | Entropia |
| Albero piccolo, pruning | Misclassification Error |
| Standard didattico | Entropia |
| Standard industriale | Gini |

# ğŸŒ³ Vantaggi e Limiti dei Decision Tree

I Decision Tree sono uno dei modelli piÃ¹ utilizzati nel machine learning classico.  
Sono potenti, flessibili e soprattutto **interpretabilissimi**.  
Ma hanno anche debolezze strutturali che spesso richiedono modelli ensemble come **Random Forest** o **Gradient Boosting** (vedi prossime lezioni).

---

# âœ… Vantaggi principali

## 1. **InterpretabilitÃ  immediata**
Ogni nodo dellâ€™albero rappresenta una decisione semplice:

> *"Petal length < 2.45?"*

Il modello puÃ² essere:
- stampato  
- disegnato  
- tradotto in regole logiche  
- spiegato a un utente non tecnico

ğŸ‘‰ Ãˆ uno dei pochi modelli ML **veramente trasparenti**.

---

## 2. **Gestiscono bene feature miste**
I Decision Tree funzionano con:
- variabili numeriche  
- variabili categoriche  
- interazioni complesse tra feature  

e non richiedono alcuna normalizzazione.

---

## 3. **Modello non parametrico**
Non assume forme particolari (lineare, quadraticaâ€¦) della relazione.  
Lâ€™albero si adatta automaticamente a decision boundary anche molto complessi.

---

# âŒ Limiti principali

## 1. **InstabilitÃ  (alta varianza)**
I Decision Tree sono estremamente sensibili ai piccoli cambiamenti nel dataset:

- aggiungi un esempio  
- rimuovi un esempio  
- cambia lâ€™ordine  

â†’ lâ€™albero puÃ² cambiare **completamente**, anche alla radice.

Questo avviene perchÃ© lo split scelto a un nodo influenza tutta la struttura sotto di esso.  
Ãˆ il problema piÃ¹ grave degli alberi singoli.

---

## 2. **Rischio elevato di overfitting**
Se non limiti profonditÃ  o numero minimo di campioni:
- lâ€™albero continua a dividere  
- isola casi specifici  
- crea regioni molto strette nel feature space  

ğŸ‘‰ fa â€œmemorizationâ€ invece di generalizzare.

---

## 3. **Decision boundary a gradini**
Il modello divide con soglie assiali:

- *feature 1 < soglia*  
- *feature 2 > soglia*  

Questo produce decision boundary a â€œscale di mattoniâ€, spesso troppo rigidi rispetto a modelli piÃ¹ flessibili.

---

## 4. **Preferenza per feature con molti valori**
Con criteri come Gini o Entropia, una feature con molti valori possibili tende ad apparire â€œmolto informativaâ€, anche quando non lo Ã¨.

Ãˆ una forma di bias strutturale (ID3/C4.5 introducono infatti il **Gain Ratio** per normalizzare questo effetto).

---

# â­ Spoiler: perchÃ© i Random Forest risolvono tutto questo?

I Random Forest nascono esattamente per eliminare i due difetti principali degli alberi singoli:

1. **InstabilitÃ **  
   â†’ lâ€™ensemble di tanti alberi â€œmedia viaâ€ le oscillazioni

2. **Overfitting**  
   â†’ ogni albero Ã¨ addestrato su un sottoinsieme dei dati e delle feature  
   â†’ la variabilitÃ  si riduce drasticamente  
   â†’ il modello diventa robusto

In pratica:

- un singolo albero ha **bassa bias**, **alta varianza**  
- un Random Forest riporta la varianza sotto controllo

ğŸ‘‰ Ecco perchÃ©, nella pratica, gli alberi â€œpuriâ€ sono soprattutto modelli didattici, mentre i Random Forest sono strumenti di lavoro.

---

# ğŸ“Œ In sintesi

| Aspetto | Decision Tree | Random Forest |
|---------|----------------|----------------|
| InterpretabilitÃ  | â­â­â­â­â­ (ottima) | â­ (bassa) |
| Robustezza | âŒ instabile | â­â­â­â­ |
| Overfitting | alto | basso |
| Performance media | discreta | ottima |
| Tempo di addestramento | molto rapido | piÃ¹ costoso |
| Uso industriale | raro da soli | diffusissimo |

Un albero Ã¨ un ottimo strumento *per capire*.  
Un Random Forest Ã¨ un ottimo strumento *per lavorare*.
"""