# -*- coding: utf-8 -*-
"""ML_Lab4_1_2025_2026_Linear_regression_VS_loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNmO_e0vbs3UldpET_avSSELNQ7Ij1q3

# üìò Machine Learning 2025/2026  

## Laboratorio 4.1 ‚Äî Regressione Lineare, Gradient Descent e Regolarizzazione  
## Docenti: Danilo Croce, Giorgio Gambosi  

In questo laboratorio esploriamo **l‚Äôintero percorso della regressione lineare**, partendo dalle basi fino alle versioni regolarizzate (*Ridge* e *Lasso*), passando attraverso la **discesa del gradiente**.  

L‚Äôobiettivo √® stimare una funzione non lineare (una sinusoide) a partire da pochi punti **rumorosi**, e comprendere come la complessit√† del modello e la regolarizzazione influenzino il risultato.

---

## üéØ Obiettivi didattici

1. **Comprendere la regressione lineare come modello base** di apprendimento supervisionato.  
2. **Costruire la matrice di design polinomiale** per trasformare un problema non lineare in uno lineare nei parametri.  
3. **Derivare e implementare la soluzione analitica** ai minimi quadrati:
   $$
   w^* = (\Phi^\top \Phi)^{-1} \Phi^\top t.
   $$
4. **Implementare la discesa del gradiente (Gradient Descent)** e confrontarla con la soluzione chiusa.  
5. **Osservare il trade-off bias‚Äìvarianza** al variare del grado polinomiale $M$.  
6. **Introdurre la regolarizzazione L2 (Ridge)** e L1 (Lasso) per evitare overfitting e ottenere modelli pi√π stabili.  

---

## üìö Struttura del notebook

| Sezione | Contenuto |
|----------|------------|
| **1. Setup e dataset** | Generazione dei dati rumorosi da una sinusoide. |
| **2. Matrice di design polinomiale** | Costruzione della base $[1, x, x^2, ‚Ä¶, x^M]$. |
| **3. Regressione ai minimi quadrati (forma chiusa)** | Derivazione e implementazione della soluzione analitica. |
| **4. Gradient Descent** | Approccio iterativo alla minimizzazione del costo. |
| **5. Ridge Regression** | Regolarizzazione L2 ‚Äî soluzione analitica e iterativa. |
| **6. Lasso Regression** | Regolarizzazione L1 e confronto qualitativo. |
| **7. Regressione con scikit-learn** | Mostriamo come utilizzare la libreria `scikit-learn` per implementare in modo semplice le stesse tecniche  |
---

### üß© Setup e Dataset

In questa prima sezione costruiamo il **dataset sintetico** su cui lavoreremo per tutto il laboratorio.

Simuliamo un esperimento di regressione in cui i dati sono generati da una **funzione sinusoidale**:

$$
t = \sin(2\pi x) + \varepsilon, \quad \varepsilon \sim \mathscr{N}(0, \sigma^2),
$$

dove $ \varepsilon $ rappresenta un **rumore gaussiano** che introduce incertezza nelle osservazioni.  

---

### üéØ Obiettivo

Il nostro scopo √® ricostruire la funzione sottostante (la sinusoide ‚Äúvera‚Äù) partendo da pochi punti rumorosi, analizzando:

1. **Come la complessit√† del modello** (grado polinomiale $M$) influisce sulla qualit√† del fit.  
2. **Come diversi metodi di ottimizzazione** ‚Äî forma chiusa, *gradient descent*, e regolarizzazioni (*Ridge* e *Lasso*) ‚Äî modificano la soluzione.  
3. **Come la normalizzazione dei dati** aiuta la stabilit√† numerica.

---

### ‚öôÔ∏è Cosa fa il codice seguente

- Genera un piccolo **training set** (15 punti) rumoroso.  
- Crea un **test set denso** per visualizzare la funzione continua.  
- Aggiunge rumore gaussiano per simulare misure reali.  
- Normalizza le feature per evitare instabilit√† nei calcoli.  
- Visualizza graficamente i punti campionati rispetto alla funzione vera $ \sin(2\pi x) $.
"""

# %%
# ============================================================
# LINEAR REGRESSION on a noisy sine function
# From least squares ‚Üí gradient descent ‚Üí ridge ‚Üí lasso
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, Lasso

# Impostiamo il seed per rendere i risultati riproducibili
np.random.seed(42)

# ------------------------------------------------------------
# 1Ô∏è‚É£  Generazione del dataset
# ------------------------------------------------------------

# Numero di punti nel training set (pochi, per rendere il problema ambiguo)
n_train = 15

# Numero di punti per il test (molti, per valutare il modello su tutto l'intervallo)
n_test = 200

# Deviazione standard del rumore gaussiano
sigma = 0.1

# Punti di training equispaziati tra 0 e 1
X_train = np.linspace(0, 1, n_train).reshape(-1, 1)

# Funzione di verit√†: sin(2œÄx)
# Aggiungiamo un rumore gaussiano ~ N(0, œÉ¬≤)
# ‚Üí simula dati sperimentali "realistici" (non perfettamente puliti)
t_train = np.sin(2 * np.pi * X_train) + np.random.normal(0, sigma, X_train.shape)

# Dataset di test molto pi√π denso (per visualizzare la curva continua)
X_test = np.linspace(0, 1, n_test).reshape(-1, 1)
t_test = np.sin(2 * np.pi * X_test)  # senza rumore ‚Üí funzione "vera"

# ------------------------------------------------------------
# 2Ô∏è‚É£  Normalizzazione delle feature
# ------------------------------------------------------------

# Centriamo e scaliamo X per evitare problemi di overflow/underflow
# (soprattutto per polinomi di grado elevato)
X_mean, X_std = np.mean(X_train), np.std(X_train)
X_train_n = (X_train - X_mean) / X_std
X_test_n = (X_test - X_mean) / X_std

# ------------------------------------------------------------
# 3Ô∏è‚É£  Visualizzazione dei dati
# ------------------------------------------------------------

plt.scatter(X_train, t_train, color="black", label="Training data (noisy)")
plt.plot(X_test, t_test, "g--", label="True function sin(2œÄx)")
plt.legend()
plt.title("Noisy samples from sin(2œÄx)")
plt.xlabel("x")
plt.ylabel("t")
plt.show()

"""## üßÆ Matrice delle feature polinomiali

Per poter applicare la regressione lineare a un problema *non lineare* (come la nostra sinusoide),  
costruiamo una **base polinomiale** dei dati di input.

Ogni punto $x_i$ viene trasformato in un vettore di potenze:

$$
\phi(x_i) = [\,1,\, x_i,\, x_i^2,\, \dots,\, x_i^M\,],
$$

dove $M$ √® il **grado del polinomio** scelto.

Raggruppando tutte le $n$ osservazioni otteniamo la **matrice di progetto** (*design matrix*):

$$
\Phi =
\begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^M \\
1 & x_2 & x_2^2 & \cdots & x_2^M \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^M
\end{bmatrix}
\in \mathbb{R}^{n\times (M+1)}.
$$

Ogni colonna rappresenta un ‚Äúlivello di complessit√†‚Äù del modello:
- la prima (tutta 1) serve per il **bias** $b$;
- le successive corrispondono ai termini $x$, $x^2$, $\dots$, $x^M$.

In Python, questa costruzione pu√≤ essere fatta comodamente con `np.hstack`:
"""

# %%
def polynomial_design_matrix(X, M):
    """Ritorna la matrice delle feature [1, x, x^2, ..., x^M]."""
    return np.hstack([X**i for i in range(M + 1)])

"""## üéØ Regressione ai Minimi Quadrati ‚Äî Soluzione Analitica

Una volta costruita la matrice delle feature $\Phi$,  
il modello polinomiale diventa **lineare nei parametri**:

$$
y = \Phi w,
$$

dove:
- $\Phi \in \mathbb{R}^{n\times (M+1)}$ √® la *design matrix* (ogni riga √® $\phi(x_i)$),
- $w \in \mathbb{R}^{M+1}$ contiene il bias e i coefficienti polinomiali,
- $t \in \mathbb{R}^{n}$ rappresenta i valori target osservati.

---

### üîπ Funzione di costo (Errore Quadratico Medio)

Definiamo il costo come **media dei quadrati dei residui**:

$$
E(w) = \frac{1}{2n}\|\Phi w - t\|^2
= \frac{1}{2n}(\Phi w - t)^\top(\Phi w - t),
$$

dove il fattore $\tfrac{1}{2n}$ √® solo una convenzione utile per semplificare le derivate.

---

### üîπ Derivata rispetto ai pesi

Richiamiamo la regola generale:

$$
\nabla_u\,\frac{1}{2}\|A u - b\|^2 = A^\top(Au - b).
$$

Applicandola con $A=\Phi$, $u=w$ e $b=t$, otteniamo:

$$
\nabla_w E(w) = \frac{1}{n}\,\Phi^\top(\Phi w - t).
$$

---

### üîπ Condizione di minimo

Per trovare il punto stazionario imponiamo che il gradiente sia nullo:

$$
\nabla_w E(w) = 0
\quad \Longrightarrow \quad
\Phi^\top(\Phi w - t) = 0.
$$

Da cui segue:

$$
\Phi^\top \Phi w = \Phi^\top t.
$$

Questa √® la **forma normale** (*normal equation*), un sistema lineare in $w$.

---

### üîπ Soluzione in forma chiusa

Se $\Phi^\top\Phi$ √® invertibile (colonne linearmente indipendenti),  
la soluzione ottimale √®:

$$
\boxed{w^\ast = (\Phi^\top\Phi)^{-1}\Phi^\top t.}
$$

Se invece $\Phi^\top\Phi$ √® singolare (ad esempio quando $M+1 > n$),  
si utilizza la **pseudoinversa di Moore‚ÄìPenrose**:

$$
w^\ast = \Phi^{+}t.
$$

---

### üîπ Interpretazione

- $\Phi^\top\Phi$ rappresenta la **correlazione** tra le colonne (feature).  
- $\Phi^\top t$ √® la **proiezione dei target** sullo spazio generato dalle feature.  
- L‚Äôequazione $(\Phi^\top\Phi)w=\Phi^\top t$ individua il vettore $w$ che  
  **minimizza la somma dei quadrati degli errori** tra le predizioni $\Phi w$ e i valori reali $t$.

---

üìò In sintesi: la regressione ai minimi quadrati trova la retta (o curva polinomiale)  
che approssima al meglio i dati nel senso dei minimi quadrati, fornendo una soluzione chiusa e diretta al problema di apprendimento.
"""

# %%
# ============================================================
# POLYNOMIAL LEAST SQUARES FIT
# ============================================================

def fit_least_squares(X, t, M):
    """
    Calcola i pesi ottimali w per la regressione ai minimi quadrati
    con base polinomiale di grado M.

    Parametri:
        X : array di input normalizzato (n x 1)
        t : target (n x 1)
        M : grado del polinomio
    Ritorna:
        w : pesi stimati (vettore di lunghezza M+1)
        Phi : matrice di design (n x M+1)
    """
    Phi = polynomial_design_matrix(X, M)              # costruisce [1, x, x^2, ..., x^M]
    w = np.linalg.pinv(Phi.T @ Phi) @ Phi.T @ t       # soluzione in forma chiusa: w = (Œ¶·µÄŒ¶)^(-1) Œ¶·µÄ t
    return w, Phi


def predict(X, w):
    """
    Genera le predizioni y = Œ¶ w dato un vettore di pesi w
    e un insieme di punti X (normalizzati).
    """
    Phi = polynomial_design_matrix(X, len(w) - 1)
    return Phi @ w


# ------------------------------------------------------------
# 1Ô∏è‚É£  Esempio singolo: M = 3
# ------------------------------------------------------------

M = 3
w_ls, Phi_train = fit_least_squares(X_train_n, t_train, M)
y_ls = predict(X_test_n, w_ls)

plt.scatter(X_train, t_train, color="black", label="Training data")
plt.plot(X_test, t_test, "g--", label="True function sin(2œÄx)")
plt.plot(X_test, y_ls, "r", label=f"Least Squares (M={M})")
plt.legend()
plt.title("Polynomial Least Squares Fit (M=3)")
plt.xlabel("x")
plt.ylabel("t")
plt.show()

"""### üîç Polynomial Fits for Multiple Degrees $ M $

In questa sezione osserviamo **come cambia la complessit√† del modello** aumentando il grado polinomiale $ M $ nella regressione ai minimi quadrati.

Partendo da pochi punti rumorosi generati da $ \sin(2\pi x) $, costruiamo modelli polinomiali di gradi diversi e confrontiamo le curve risultanti:

$$
y(x) = \sum_{j=0}^{M} w_j x^j = w_0 + w_1 x + w_2 x^2 + \dots + w_M x^M
$$

Per ciascun $ M $:

1. Costruiamo la **matrice di design** $ \Phi = [1, x, x^2, \dots, x^M] $.
2. Calcoliamo i pesi ottimali $ w = (\Phi^\top \Phi)^{-1} \Phi^\top t $.
3. Visualizziamo la curva ottenuta insieme ai dati e alla funzione vera.

---

"""

# %%
# ============================================================
# 2Ô∏è‚É£  POLYNOMIAL FITS FOR MULTIPLE DEGREES M + MSE
# ============================================================

from sklearn.metrics import mean_squared_error

degrees = [0, 1, 3, 5, 9, 12]  # diversi gradi di complessit√†

plt.figure(figsize=(12, 8))

print("=== Mean Squared Error (MSE) ===")
print("Grado M\tTrain MSE\tTest MSE")
print("----------------------------------")

for M in degrees:
    # Fit del modello ai minimi quadrati
    w_ls, Phi_train = fit_least_squares(X_train_n, t_train, M)

    # Predizioni su train e test
    y_train_pred = predict(X_train_n, w_ls)
    y_test_pred  = predict(X_test_n, w_ls)

    # Calcolo MSE
    mse_train = mean_squared_error(t_train, y_train_pred)
    mse_test  = mean_squared_error(t_test, y_test_pred)

    # Stampa tabellina dei risultati
    print(f"{M:<7d}\t{mse_train:.5f}\t{mse_test:.5f}")

    # Tracciamento del modello
    plt.plot(X_test, y_test_pred, label=f"M={M} (MSE={mse_test:.4f})")

# Dati e funzione vera
plt.scatter(X_train, t_train, color="black", s=40, label="Training data")
plt.plot(X_test, t_test, "g--", linewidth=2, label="True function sin(2œÄx)")

plt.legend()
plt.title("Polynomial Least Squares ‚Äî Underfitting ‚Üí Overfitting")
plt.xlabel("x")
plt.ylabel("t")
plt.show()

"""#### üí° Cosa osservare nel grafico

- **Basso grado (M = 0, 1)** ‚Üí  
  La curva √® troppo semplice: non riesce a seguire la forma sinusoidale.  
  ‚Üí **Underfitting**: alto bias, errore sistematico.

- **Grado medio (M = 3‚Äì5)** ‚Üí  
  Il modello segue bene l‚Äôandamento della sinusoide senza oscillare.  
  ‚Üí Buon compromesso bias‚Äìvarianza.

- **Grado alto (M ‚â• 9)** ‚Üí  
  Il modello ‚Äúpassa per tutti i punti di training‚Äù, ma oscilla violentemente tra di essi.  
  ‚Üí **Overfitting**: bassa capacit√† di generalizzare.

---

#### üìà Conclusione

Aumentare $ M $ fa crescere la **flessibilit√† del modello**, ma anche la **varianza** delle previsioni.  
Con pochi dati e rumore, gradi troppo alti portano a curve irrealistiche: il modello impara anche il rumore invece del segnale.

Questo esperimento visualizza in modo chiaro il **trade-off bias‚Äìvarianza**, cuore di tutti i problemi di apprendimento automatico.

---

## ‚öôÔ∏è Polynomial Regression ‚Äî Gradient Descent Implementation

Finora abbiamo risolto la regressione polinomiale **in forma chiusa**, con la classica soluzione ai minimi quadrati:

$$
w^\ast = (\Phi^\top \Phi)^{-1} \Phi^\top t.
$$

Ora affrontiamo lo stesso problema **in modo iterativo**, usando la **Discesa del Gradiente (Gradient Descent)**,  
che aggiorna progressivamente i parametri per ridurre l‚Äôerrore.

---

### üîπ 1Ô∏è‚É£ Funzione di costo

Definiamo la funzione di costo come l‚Äô**errore quadratico medio (MSE)**:

$$
E(w) = \frac{1}{2n} \|\Phi w - t\|^2
     = \frac{1}{2n}(\Phi w - t)^\top(\Phi w - t).
$$

Il fattore $\tfrac{1}{2n}$ √® solo una costante di normalizzazione utile per semplificare le derivate.

---

### üîπ 2Ô∏è‚É£ Gradiente rispetto a $w$

Calcoliamo il gradiente $\nabla_w E(w)$.

- Derivando il termine quadratico otteniamo:  
  $$\nabla_w \frac{1}{2n} w^\top \Phi^\top \Phi w = \frac{1}{n}\Phi^\top \Phi w.$$

- Il termine misto:  
  $$\nabla_w \left(-\frac{1}{n} t^\top \Phi w \right) = -\frac{1}{n}\Phi^\top t.$$

- Il termine $t^\top t$ non dipende da $w$ ‚Üí derivata nulla.

Combinando tutto:
$$
\nabla_w E(w) = \frac{1}{n}\Phi^\top(\Phi w - t).
$$

---

### üîπ 3Ô∏è‚É£ Aggiornamento iterativo

Ad ogni passo $k$:

$$
w^{(k+1)} = w^{(k)} - \eta \, \nabla_w E(w^{(k)}),
$$

ossia:

$$
w^{(k+1)} = w^{(k)} - \frac{\eta}{n}\Phi^\top(\Phi w^{(k)} - t),
$$

dove $\eta$ √® il **learning rate**, cio√® la dimensione del passo di aggiornamento.

---

### üîπ 4Ô∏è‚É£ Interpretazione intuitiva

- $(\Phi w - t)$ √® il vettore dei **residui** (gli errori di predizione).  
- $\Phi^\top(\Phi w - t)$ calcola quanto questi errori sono **correlati con ciascuna feature**.  
- Aggiornando $w$ nella direzione opposta, il modello ‚Äúraddrizza‚Äù progressivamente la previsione finch√© il gradiente si annulla.

---

### üîπ 5Ô∏è‚É£ Convergenza e minimo

Quando il gradiente diventa nullo:

$$
\nabla_w E(w^\ast) = 0
\;\;\Rightarrow\;\;
\Phi^\top(\Phi w^\ast - t) = 0,
$$

che √® **esattamente** la condizione dei **minimi quadrati**:

$$
(\Phi^\top \Phi)w^\ast = \Phi^\top t.
$$

---

### üîπ 6Ô∏è‚É£ Considerazioni pratiche

| Caso | Effetto |
|------|----------|
| $\eta$ troppo grande | Il costo oscilla o diverge |
| $\eta$ troppo piccolo | Converge molto lentamente |
| Valore ideale | Diminuzione rapida ma stabile di $E(w)$ |

Il grafico ‚Äú**Costo vs Iterazione**‚Äù mostra tipicamente una discesa rapida iniziale, poi un plateau vicino al minimo.

---

### üí° In sintesi

| Concetto | Formula | Significato |
|-----------|----------|-------------|
| Funzione di costo | $${E(w) = \tfrac{1}{2n}\|\Phi w - t\|^2}$$ | Errore medio quadratico |
| Gradiente | $$\nabla_w E(w) = \tfrac{1}{n}\Phi^\top(\Phi w - t)$$ | Direzione di massima crescita |
| Aggiornamento | $$w \leftarrow w - \eta \nabla_w E(w)$$ | Passo verso il minimo |
| Condizione di minimo | $$\Phi^\top(\Phi w^\ast - t)=0$$ | Coincide con la soluzione ai minimi quadrati |

---

### üîπ 7Ô∏è‚É£ Collegamento al codice

Nel notebook, implementiamo:
- `cost()` ‚Üí calcola $E(w)$  
- `grad()` ‚Üí calcola $\nabla_w E(w)$  
- `gradient_descent()` ‚Üí aggiorna $w$ iterativamente  

Alla fine, $w$ converge alla stessa soluzione analitica,  
ma ottenuta **tramite aggiornamenti incrementali**, come nei moderni algoritmi di *deep learning*.
"""

# %%
# ============================================================
# POLYNOMIAL REGRESSION ‚Äî GRADIENT DESCENT (BATCH FORM)
# ============================================================
# Obiettivo:
#   Approssimare una funzione (sinusoidale) usando regressione polinomiale
#   con ottimizzazione iterativa tramite discesa del gradiente.
#
#   In questa versione il gradiente √® calcolato su TUTTO il dataset
#   ‚Üí "Batch Gradient Descent".
#   Le versioni "Stochastic" e "Mini-Batch" sono varianti che usano
#   rispettivamente un solo campione o un sottoinsieme a ogni passo.
# ============================================================

import numpy as np
import matplotlib.pyplot as plt


# ------------------------------------------------------------
# 1Ô∏è‚É£  Funzione di costo (Mean Squared Error)
# ------------------------------------------------------------
def cost(Phi, w, t):
    """
    Calcola la funzione di costo:
        E(w) = 1/(2n) * ||Œ¶w - t||¬≤
    dove:
        Œ¶ : matrice di design (n √ó M+1)
        w : vettore dei pesi (M+1 √ó 1)
        t : target (n √ó 1)
    """
    r = Phi @ w - t        # residui = differenza tra predizioni e valori reali
    return 0.5 * np.mean(r**2)


# ------------------------------------------------------------
# 2Ô∏è‚É£  Gradiente della funzione di costo
# ------------------------------------------------------------
def grad(Phi, w, t):
    """
    Calcola il gradiente del costo rispetto ai pesi:
        ‚àáE(w) = (1/n) * Œ¶·µÄ(Œ¶w - t)
    che indica la direzione di massima crescita del costo.
    """
    r = Phi @ w - t
    return (Phi.T @ r) / len(t)


# ------------------------------------------------------------
# 3Ô∏è‚É£  Discesa del gradiente (Batch)
# ------------------------------------------------------------
def gradient_descent(Phi, t, eta=1e-2, epochs=5000):
    """
    Esegue la discesa del gradiente batch:
        w ‚Üê w - Œ∑ ‚àáE(w)
    fino alla convergenza o per un numero prefissato di iterazioni.

    Parametri:
        Phi    : matrice di design (n √ó M+1)
        t      : vettore target (n √ó 1)
        eta    : learning rate (controlla l‚Äôampiezza del passo)
        epochs : numero di iterazioni di aggiornamento

    Ritorna:
        w      : pesi appresi
        costs  : lista dei valori di E(w) nel tempo
    """
    # Inizializzazione dei pesi (tutti a zero)
    w = np.zeros((Phi.shape[1], 1))
    costs = []

    # Ciclo di ottimizzazione
    for _ in range(epochs):
        # Calcolo del gradiente completo (su tutti i dati)
        g = grad(Phi, w, t)
        # Aggiornamento dei pesi
        w -= eta * g
        # Calcolo e memorizzazione del costo corrente
        costs.append(cost(Phi, w, t))

    return w, costs


# ------------------------------------------------------------
# 4Ô∏è‚É£  Applicazione: POLINOMIO DI GRADO M = 3
# ------------------------------------------------------------
M = 3
Phi_train = polynomial_design_matrix(X_train_n, M)

# Applichiamo la discesa del gradiente
# Œ∑ = 0.1 ‚Üí learning rate moderato
# epochs = 2000 ‚Üí iterazioni sufficienti per convergere
w_gd, costs = gradient_descent(Phi_train, t_train, eta=0.1, epochs=2000)

# Predizioni sul test set
y_gd = predict(X_test_n, w_gd)


# ------------------------------------------------------------
# 5Ô∏è‚É£  Analisi della convergenza
# ------------------------------------------------------------
plt.plot(costs)
plt.title("Gradient Descent ‚Äî Costo vs Iterazione")
plt.xlabel("Iterazione")
plt.ylabel("Costo (MSE)")
plt.grid(True)
plt.show()


# ------------------------------------------------------------
# 6Ô∏è‚É£  Confronto con la funzione vera
# ------------------------------------------------------------
plt.scatter(X_train, t_train, color="black", label="Training data")
plt.plot(X_test, t_test, "g--", label="True function sin(2œÄx)")
plt.plot(X_test, y_gd, "r", label=f"Gradient Descent (M={M})")
plt.legend()
plt.title("Polynomial Fit via Gradient Descent (no regularization)")
plt.xlabel("x")
plt.ylabel("t")
plt.show()


# ------------------------------------------------------------
# üî∏ NOTA: VARIANTI DEL GRADIENTE üî∏
# ------------------------------------------------------------
# - BATCH GRADIENT DESCENT:
#     Usa TUTTO il dataset per calcolare ‚àáE(w) a ogni passo.
#     ‚Üí Aggiornamento preciso ma costoso (1 passo per epoca).
#
# - STOCHASTIC GRADIENT DESCENT (SGD):
#     Usa UN SOLO campione (x_i, t_i) per calcolare il gradiente:
#         g_i = (Œ¶_i·µÄ(Œ¶_i w - t_i))
#     ‚Üí Aggiornamento molto frequente, ma rumoroso.
#
# - MINI-BATCH GRADIENT DESCENT:
#     Usa un sottoinsieme casuale B ‚äÇ {1,‚Ä¶,n}:
#         g_B = (1/|B|) Œ¶_B·µÄ(Œ¶_B w - t_B)
#     ‚Üí Compromesso tra stabilit√† (batch) e velocit√† (SGD).
#
# Queste versioni si implementano modificando la riga:
#     g = grad(Phi, w, t)
# sostituendola con il calcolo del gradiente su un sottoinsieme.

"""### ‚ö†Ô∏è Nota sui gradi polinomiali elevati

Nel codice abbiamo limitato i gradi polinomiali a $M \le 7$,  
evitando valori pi√π alti (come $M=9$ o $M=12$) per motivi di **stabilit√† numerica**.

Infatti, la matrice di design

$$
\Phi = [1, x, x^2, \dots, x^M]
$$

contiene potenze crescenti di $x$, e quando $x$ non √® perfettamente centrato o normalizzato,  
i termini $x^M$ possono diventare **molto grandi** o **molto piccoli** (overflow o underflow numerico).

Questo comporta:
- una **condizionatura molto alta** della matrice $\Phi^\top \Phi$,  
  che rende l‚Äôapprendimento instabile;
- una propagazione degli errori di arrotondamento in `float64`;
- gradienti che diventano enormi o NaN durante l‚Äôottimizzazione.

Per questo motivo, nei test sperimentali abbiamo scelto gradi moderati  
($M = 0, 1, 3, 5, 7$), che permettono di osservare il fenomeno  
di *underfitting ‚Üí overfitting* senza incorrere in problemi di rappresentazione numerica.

---

## üöÄ Polynomial Regression con Gradient Descent

Applichiamo la **discesa del gradiente** per stimare i parametri $w$ di modelli polinomiali di diverso grado $M$.  
Per ciascun $M$:
- costruiamo la matrice di design $\Phi = [1, x, x^2, ‚Ä¶, x^M]$  
- aggiorniamo i pesi iterativamente $w \leftarrow w - \eta \nabla_w E(w)$  
- tracciamo la curva appresa e l‚Äôandamento del costo $E(w)$.

In questo modo osserviamo come la complessit√† del modello e il learning rate influenzano  
la **convergenza** e il **rischio di overfitting**.
"""

# %%
# ============================================================
# 3Ô∏è‚É£  POLYNOMIAL FITS (GRADIENT DESCENT VERSION)
# ============================================================
# Per ogni grado M:
#   - costruiamo la matrice di design Œ¶
#   - addestriamo il modello con discesa del gradiente
#   - salviamo l'andamento del costo
#   - visualizziamo le curve apprese
# ============================================================

degrees = [0, 1, 3, 5, 7]      # diversi gradi di complessit√†
eta = 0.01                          # learning rate moderato
epochs = 5000                     # numero di iterazioni

plt.figure(figsize=(12, 8))

loss_histories = {}  # per tracciare i costi durante l'addestramento

# ------------------------------------------------------------
# Ciclo sui diversi gradi polinomiali
# ------------------------------------------------------------
for M in degrees:
    # Costruzione della matrice di design
    Phi_train = polynomial_design_matrix(X_train_n, M)

    # Addestramento tramite discesa del gradiente
    w_gd, costs = gradient_descent(Phi_train, t_train, eta=eta, epochs=epochs)

    # Predizioni sul test set
    y_gd = predict(X_test_n, w_gd)

    # Tracciamo la curva appresa
    plt.plot(X_test, y_gd, label=f"M={M}")

    # Salviamo la loss
    loss_histories[M] = costs

# ------------------------------------------------------------
# Dati e funzione vera
# ------------------------------------------------------------
plt.scatter(X_train, t_train, color="black", s=40, label="Training data")
plt.plot(X_test, t_test, "g--", linewidth=2, label="True function sin(2œÄx)")

plt.legend()
plt.title("Polynomial Regression (Gradient Descent) ‚Äî Underfitting ‚Üí Overfitting")
plt.xlabel("x")
plt.ylabel("t")
plt.show()


# ------------------------------------------------------------
# 4Ô∏è‚É£  Andamento della funzione di costo per ogni grado M
# ------------------------------------------------------------
plt.figure(figsize=(10, 6))
for M, costs in loss_histories.items():
    plt.plot(costs, label=f"M={M}")
plt.title("Convergenza del Costo ‚Äî Gradient Descent per vari M")
plt.xlabel("Iterazione")
plt.ylabel("Costo (MSE)")
plt.legend()
plt.grid(True)
plt.show()

"""---

## ‚öôÔ∏è Ridge Regression ‚Äî Soluzione in forma chiusa

La **Ridge Regression** (o **Least Squares regolarizzata**) √® un‚Äôestensione della regressione lineare classica, pensata per:

1. **Evitare l‚Äôoverfitting**, specialmente quando il grado del polinomio $M$ √® alto.  
2. **Gestire problemi di multicollinearit√†** (colonne di $\Phi$ quasi dipendenti).  
3. **Stabilizzare la matrice** $(\Phi^\top \Phi)$, rendendola invertibile.

---

### üîπ 1Ô∏è‚É£ Funzione di costo con regolarizzazione L2

A differenza dei minimi quadrati standard, aggiungiamo una penalizzazione sulla norma dei pesi:

$$
E_\lambda(w) = \frac{1}{2n} \| \Phi w - t \|^2 + \frac{\lambda}{2} \| w \|^2
$$

dove:

- $\Phi$ √® la matrice di design $(n \times (M+1))$,  
- $w$ √® il vettore dei coefficienti,  
- $\lambda > 0$ √® il **fattore di regolarizzazione**.  

> Il secondo termine spinge i pesi $w$ a rimanere piccoli, riducendo la complessit√† del modello.

---

### üîπ 2Ô∏è‚É£ Derivazione del gradiente

Partiamo dal costo:

$$
E_\lambda(w) = \frac{1}{2n}(\Phi w - t)^\top(\Phi w - t) + \frac{\lambda}{2} w^\top w.
$$

Calcoliamo la derivata rispetto a $w$:

$$
\begin{aligned}
\nabla_w E_\lambda(w)
&= \frac{1}{n}\Phi^\top(\Phi w - t) + \lambda w.
\end{aligned}
$$

---

### üîπ 3Ô∏è‚É£ Condizione di minimo

Al minimo, il gradiente √® nullo:

$$
\nabla_w E_\lambda(w^\ast) = 0
\quad \Rightarrow \quad
\frac{1}{n}\Phi^\top(\Phi w^\ast - t) + \lambda w^\ast = 0.
$$

Moltiplichiamo per $n$ e riordiniamo:

$$
(\Phi^\top \Phi + n\lambda I) w^\ast = \Phi^\top t.
$$

> In molti testi (come Bishop o Hastie), $\lambda$ viene ridefinito assorbendo il fattore $n$:
>
> $$
> (\Phi^\top \Phi + \lambda I) w^\ast = \Phi^\top t.
> $$

Questa √® la **forma chiusa della Ridge Regression**.

---

### üîπ 4Ô∏è‚É£ Soluzione analitica

Risolvendo il sistema lineare otteniamo:

$$
\boxed{
w^\ast = (\Phi^\top \Phi + \lambda I)^{-1} \Phi^\top t.
}
$$

- Se $\lambda = 0$, si torna ai **minimi quadrati ordinari**.  
- Se $\lambda > 0$, la matrice da invertire √® **sempre ben condizionata** (invertibile anche in caso di collinearit√†).  
- $\lambda$ controlla il *trade-off* bias‚Äìvarianza:
  - $\lambda$ grande ‚Üí pesi piccoli, modello rigido (pi√π bias, meno varianza);  
  - $\lambda$ piccolo ‚Üí modello flessibile, pi√π rischio di overfitting.

---

### üîπ 5Ô∏è‚É£ Interpretazione geometrica

L‚Äôaggiunta del termine $\lambda \|w\|^2$ pu√≤ essere vista come una **penalizzazione**:

- senza regolarizzazione, i pesi $w$ possono crescere molto per adattarsi ai punti rumorosi;  
- con regolarizzazione, il modello √® ‚Äútirato‚Äù verso la soluzione pi√π semplice (pesi piccoli).

---

### üîπ 6Ô∏è‚É£ Effetto pratico nel codice

Nel codice, la riga chiave:

$$
w = (\Phi^\top \Phi + \lambda I)^{-1} \Phi^\top t
$$

implementa esattamente questa formula.

- `I` √® la matrice identit√† $(M+1) \times (M+1)$,  
- il termine `lam * I` regolarizza la diagonale,  
- `np.linalg.pinv()` assicura stabilit√† numerica anche se la matrice non √® perfettamente invertibile.

---

### üí° In sintesi

| Concetto | Formula | Effetto |
|-----------|----------|---------|
| Funzione di costo | $E_\lambda(w) = \frac{1}{2n}\|\Phi w - t\|^2 + \frac{\lambda}{2}\|w\|^2$ | penalizza pesi grandi |
| Gradiente | $\nabla_w E_\lambda(w) = \frac{1}{n}\Phi^\top(\Phi w - t) + \lambda w$ | aggiunge ‚Äúforza di richiamo‚Äù |
| Equazione normale | $$(\Phi^\top \Phi + \lambda I)w = \Phi^\top t$$ | sistema lineare regolarizzato |
| Soluzione chiusa | $$w^\ast = (\Phi^\top \Phi + \lambda I)^{-1}\Phi^\top t$$ | sempre invertibile |
| Effetto di $\lambda$ | ‚Üë stabilit√†, ‚Üì varianza, ‚Üë bias | controlla la complessit√† |

---

In pratica, **Ridge Regression** √® una versione pi√π robusta dei minimi quadrati:
mantiene le propriet√† lineari, ma limita l‚Äôampiezza dei coefficienti, ottenendo modelli pi√π **stabili e generalizzabili**, specialmente con dati rumorosi o feature correlate.
"""

# %%
# ============================================================
# RIDGE REGRESSION ‚Äî Closed-form implementation
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

def fit_ridge(X, t, M, lam):
    """
    Stima i pesi w della Ridge Regression:
        w = (Œ¶·µÄŒ¶ + ŒªI)‚Åª¬π Œ¶·µÄt
    Parametri:
        X   : input normalizzato (n√ó1)
        t   : target (n√ó1)
        M   : grado del polinomio
        lam : coefficiente di regolarizzazione Œª
    Ritorna:
        w   : pesi stimati
        Œ¶   : matrice di design
    """
    # Costruzione della matrice di design
    Phi = polynomial_design_matrix(X, M)

    # Matrice identit√† per il termine ŒªI
    I = np.eye(Phi.shape[1])

    # Soluzione chiusa della Ridge
    w = np.linalg.pinv(Phi.T @ Phi + lam * I) @ Phi.T @ t

    return w, Phi


# ------------------------------------------------------------
# 1Ô∏è‚É£  Esperimento con M = 9 (modello molto flessibile)
# ------------------------------------------------------------
M = 9
lam = 1e-2  # regolarizzazione moderata

# Fit del modello Ridge
w_ridge, Phi_train = fit_ridge(X_train_n, t_train, M, lam)

# Predizioni
y_ridge_train = predict(X_train_n, w_ridge)
y_ridge_test = predict(X_test_n, w_ridge)

# ------------------------------------------------------------
# 2Ô∏è‚É£  Confronto visivo
# ------------------------------------------------------------
plt.scatter(X_train, t_train, color="black", label="Training data")
plt.plot(X_test, t_test, "g--", label="True function sin(2œÄx)")
plt.plot(X_test, y_ridge_test, "r", label=f"Ridge Œª={lam}")
plt.legend()
plt.title(f"Ridge Regression (Closed-form) ‚Äî Degree M={M}")
plt.xlabel("x")
plt.ylabel("t")
plt.show()


# ------------------------------------------------------------
# 3Ô∏è‚É£  Valutazione quantitativa
# ------------------------------------------------------------
mse_train = mean_squared_error(t_train, y_ridge_train)
mse_test = mean_squared_error(t_test, y_ridge_test)

print(f"Œª = {lam}")
print(f"MSE (train): {mse_train:.4f}")
print(f"MSE (test) : {mse_test:.4f}")

"""## üîç Effetto della regolarizzazione Œª in Ridge Regression

In questa sezione osserviamo come la **regolarizzazione L2** (controllata dal parametro Œª)  
influenzi il comportamento del modello polinomiale.

- **Œª = 0** ‚Üí coincide con la regressione ai minimi quadrati: il modello segue perfettamente i dati (rischio di *overfitting*).  
- **Œª moderato (10‚Åª¬≥ ‚Äì 10‚Åª¬π)** ‚Üí i pesi vengono ‚Äúcontenuti‚Äù, il modello √® pi√π liscio e generalizza meglio.  
- **Œª grande (‚â• 1)** ‚Üí la penalit√† domina: la curva diventa quasi lineare (*underfitting*).

üìà In questo esperimento visualizziamo l‚Äôeffetto di diversi valori di Œª,  
mantenendo fisso il grado polinomiale $M=9$.
"""

# %%
# ============================================================
# RIDGE REGRESSION ‚Äî Effetto della regolarizzazione Œª
# ============================================================

from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Parametri di esperimento
M = 9                         # grado polinomiale elevato ‚Üí alto rischio di overfitting
lambdas = [0, 1e-4, 1e-2, 1e-1, 1]   # diversi livelli di regolarizzazione

plt.figure(figsize=(12, 8))

# ------------------------------------------------------------
# Ciclo sui diversi valori di Œª
# ------------------------------------------------------------
for lam in lambdas:
    # Fit del modello Ridge (Œª = 0 equivale ai minimi quadrati)
    w_ridge, Phi_train_ridge = fit_ridge(X_train_n, t_train, M, lam)
    y_ridge_test = predict(X_test_n, w_ridge)

    # Calcolo dell'errore sul test set
    mse_test = mean_squared_error(t_test, y_ridge_test)

    # Tracciamento della curva
    label = f"Œª={lam}  (MSE={mse_test:.3f})" if lam > 0 else f"Least Squares (Œª=0)"
    plt.plot(X_test, y_ridge_test, linewidth=2, label=label)

# ------------------------------------------------------------
# Dati e funzione vera
# ------------------------------------------------------------
plt.scatter(X_train, t_train, color="black", s=40, label="Training data")
plt.plot(X_test, t_test, "g--", linewidth=2, label="True function sin(2œÄx)")

plt.title(f"Effetto della regolarizzazione Ridge (grado polinomiale M={M})")
plt.xlabel("x")
plt.ylabel("t")
plt.legend()
plt.show()

"""## Ridge via Gradient Descent

## ‚öôÔ∏è Ridge Regression ‚Äî Gradient Descent Implementation (con Clipping)

In questa sezione implementiamo la **Ridge Regression** in forma **iterativa**,
utilizzando la **Discesa del Gradiente batch (Batch Gradient Descent)**,  
invece della classica soluzione chiusa dei minimi quadrati regolarizzati.

---

### üîπ 1Ô∏è‚É£ Funzione di costo regolarizzata

La funzione di costo Ridge combina due termini:

$$
E_\lambda(w) = \frac{1}{2n}\|\Phi w - t\|^2 + \frac{\lambda}{2}\|w\|^2
$$

- **Errore di ricostruzione**: misura quanto bene il modello approssima i dati  
- **Penalit√† L2**: riduce i pesi troppo grandi, controllando la complessit√† del modello  

La Ridge, infatti, tende a preferire soluzioni ‚Äúpiccole‚Äù ma stabili.

---

### üîπ 2Ô∏è‚É£ Discesa del gradiente batch

Il gradiente della funzione di costo √®:

$$
\nabla_w E_\lambda(w) = \frac{1}{n}\Phi^\top(\Phi w - t) + \lambda w
$$

e l‚Äôaggiornamento dei pesi ad ogni iterazione √®:

$$
w \leftarrow w - \eta \, \nabla_w E_\lambda(w)
$$

dove:
- $ \eta $ √® il **learning rate**,  
- $ \lambda $ controlla la **forza della regolarizzazione**.  

Dopo un numero sufficiente di epoche, $w$ converge al valore ottimo che minimizza $E_\lambda(w)$ (equivalente alla soluzione chiusa della Ridge).

---

### üîπ 3Ô∏è‚É£ Clipping: perch√© lo usiamo

Nel codice, sia i **residui** $(\Phi w - t)$ sia i **gradienti** vengono ‚Äúclipped‚Äù con la funzione `np.clip()` per **limitare i valori estremi**.

Questo serve a **prevenire instabilit√† numeriche**:

- nei modelli polinomiali, i valori di $\Phi$ possono crescere molto rapidamente (es. $x^9$, $x^{12}$‚Ä¶),  
- di conseguenza, il gradiente pu√≤ assumere valori enormi, causando divergenza o overflow.

Il **gradient clipping** forza il gradiente (o i pesi) a rimanere entro un intervallo controllato:

$$
g_i \leftarrow \text{clip}(g_i, -c, +c)
$$

Questo √® lo stesso principio usato nel **deep learning** per stabilizzare l‚Äôaddestramento.

> üí° In breve: il clipping non modifica il minimo teorico, ma evita che il processo di discesa ‚Äúesploda‚Äù durante l‚Äôottimizzazione.

---

### üîπ 4Ô∏è‚É£ Interpretazione dei risultati

- Il grafico del costo $E_\lambda(w)$ decresce gradualmente fino a stabilizzarsi ‚Üí **convergenza**.  
- La curva appresa (linea rossa) √® **pi√π regolare** della regressione ai minimi quadrati puri:  
  la regolarizzazione $\lambda$ ha effettivamente ridotto l‚Äôoverfitting.  
- All‚Äôaumentare di $\lambda$, i pesi si contraggono e la curva diventa pi√π liscia.

---

### üîπ 5Ô∏è‚É£ Metrica di valutazione

Infine, confrontiamo il **Mean Squared Error (MSE)** su training e test set:

- MSE basso su entrambi ‚Üí buon fit generalizzante  
- MSE basso su train ma alto su test ‚Üí *overfitting*  
- MSE alto su entrambi ‚Üí *underfitting*

---

### üí° In sintesi

| Concetto | Formula | Effetto |
|-----------|----------|---------|
| Costo Ridge | $$E_\lambda(w) = \tfrac{1}{2n}\|\Phi w - t\|^2 + \tfrac{\lambda}{2}\|w\|^2$$ | Penalizza pesi grandi |
| Gradiente | $$\nabla_w E_\lambda(w) = \tfrac{1}{n}\Phi^\top(\Phi w - t) + \lambda w$$ | Direzione di discesa |
| Aggiornamento | $w \leftarrow w - \eta \nabla_w E_\lambda(w)$ | Passo iterativo verso il minimo |
| Clipping | `np.clip(g, -c, +c)` | Stabilizza l‚Äôottimizzazione |
| Risultato | MSE pi√π basso e curva pi√π liscia | Migliore generalizzazione |

---

üìà **Osservazione finale:**  
Questa versione iterativa con clipping √® concettualmente la stessa che si usa per allenare reti neurali ‚Äî  
solo che qui la usiamo su un semplice modello di regressione lineare,  
rendendo evidente il legame tra **Ridge Regression** e **ottimizzazione numerica moderna**.
"""

# %%
# ============================================================
# RIDGE REGRESSION ‚Äî Gradient Descent Implementation (con clipping)
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

# ------------------------------------------------------------
# 1Ô∏è‚É£  Funzione di costo e gradiente
# ------------------------------------------------------------
def cost_ridge(Phi, w, t, lam):
    """
    Funzione di costo regolarizzata (Ridge):
        EŒª(w) = 1/(2n) * ||Œ¶w - t||¬≤ + (Œª/2) * ||w||¬≤
    """
    r = Phi @ w - t
    mse_term = 0.5 * np.mean(r**2)
    reg_term = 0.5 * lam * np.sum(w**2)
    return mse_term + reg_term


def grad_ridge(Phi, w, t, lam):
    """
    Gradiente della funzione di costo Ridge:
        ‚àáEŒª(w) = (1/n) * Œ¶·µÄ(Œ¶w - t) + Œªw
    """
    r = Phi @ w - t
    return (Phi.T @ r) / len(t) + lam * w


# ------------------------------------------------------------
# 2Ô∏è‚É£  Discesa del gradiente batch
# ------------------------------------------------------------
def gradient_descent_ridge(Phi, t, lam=1e-2, eta=1e-2, epochs=3000,
                           clip_grad=None, clip_w=None):
    """
    Discesa del gradiente per Ridge Regression.
    Include il clipping opzionale di gradienti e pesi.
    """
    w = np.zeros((Phi.shape[1], 1))
    costs = []

    for _ in range(epochs):
        # Gradiente Ridge
        g = grad_ridge(Phi, w, t, lam)

        # Gradient clipping (per stabilit√† numerica)
        if clip_grad is not None:
            g = np.clip(g, -clip_grad, clip_grad)

        # Aggiornamento dei pesi
        w -= eta * g

        # (Opzionale) Clipping dei pesi
        if clip_w is not None:
            w = np.clip(w, -clip_w, clip_w)

        # Calcolo del costo
        costs.append(cost_ridge(Phi, w, t, lam))

    return w, costs


# ------------------------------------------------------------
# 3Ô∏è‚É£  Esperimento: confronto visivo e andamento della loss
# ------------------------------------------------------------
M = 7         # grado polinomiale
lam = 0.1     # regolarizzazione
eta = 0.01    # learning rate
epochs = 5000 # numero di iterazioni
clip_grad = 1 # soglia di clipping del gradiente
clip_w = None # opzionale: clipping dei pesi

# Matrice di design
Phi_train = polynomial_design_matrix(X_train_n, M)

# Addestramento con discesa del gradiente Ridge
w_ridge_gd, costs = gradient_descent_ridge(Phi_train, t_train, lam, eta, epochs,
                                           clip_grad=clip_grad, clip_w=clip_w)

# Predizioni su train e test
y_ridge_train = predict(X_train_n, w_ridge_gd)
y_ridge_test  = predict(X_test_n, w_ridge_gd)

# ------------------------------------------------------------
# 4Ô∏è‚É£  Grafico della funzione di costo
# ------------------------------------------------------------
plt.figure(figsize=(8,4))
plt.plot(costs)
plt.title(f"Ridge Regression ‚Äî Costo vs Iterazione (Œª={lam}, Œ∑={eta}, clip={clip_grad})")
plt.xlabel("Iterazione")
plt.ylabel("Costo EŒª(w)")
plt.grid(True)
plt.show()

# ------------------------------------------------------------
# 5Ô∏è‚É£  Confronto visivo del fit
# ------------------------------------------------------------
plt.figure(figsize=(8,5))
plt.scatter(X_train, t_train, color="black", label="Training data")
plt.plot(X_test, t_test, "g--", label="True function sin(2œÄx)")
plt.plot(X_test, y_ridge_test, "r", linewidth=2, label="Ridge (GD)")
plt.legend()
plt.title(f"Ridge Regression via Gradient Descent (M={M}, Œª={lam}, clip={clip_grad})")
plt.xlabel("x")
plt.ylabel("t")
plt.show()

# ------------------------------------------------------------
# 6Ô∏è‚É£  Valutazione numerica
# ------------------------------------------------------------
mse_train = mean_squared_error(t_train, y_ridge_train)
mse_test  = mean_squared_error(t_test, y_ridge_test)
print(f"MSE train: {mse_train:.4f}")
print(f"MSE test : {mse_test:.4f}")

"""## üß∞ Regressione lineare con *scikit-learn*

Finora abbiamo derivato e implementato a mano:
- la **regressione ai minimi quadrati**,  
- la **Ridge Regression** (regolarizzazione L2),  
- e la **Lasso Regression** (regolarizzazione L1).

Tutti questi modelli sono disponibili direttamente nella libreria **`scikit-learn`**,  
che fornisce implementazioni ottimizzate e numericamente stabili.

Vediamo come usare:
- `LinearRegression` ‚Üí minimi quadrati standard,  
- `Ridge` ‚Üí regressione L2,  
- `Lasso` ‚Üí regressione L1.
"""

# %%
# ============================================================
# RIDGE REGRESSION ‚Äî Effetto combinato di grado M e regolarizzazione Œª
# ============================================================

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Gradi polinomiali da esplorare
degrees = [3, 5, 9, 12, 15]
# Diversi valori di regolarizzazione
lambdas = [0, 1e-3, 1e-2, 1e-1, 1]

# Creazione dei subplot (una riga per ogni M)
fig, axes = plt.subplots(len(degrees), 1, figsize=(10, 14), sharex=True)

# Dizionario per salvare i risultati numerici
mse_results = {}

for i, M in enumerate(degrees):
    ax = axes[i]
    Phi_train = polynomial_design_matrix(X_train_n, M)
    Phi_test = polynomial_design_matrix(X_test_n, M)
    mse_results[M] = {}

    for lam in lambdas:
        # Addestra il modello Ridge
        ridge = Ridge(alpha=lam, fit_intercept=False)
        ridge.fit(Phi_train, t_train)

        # Predizioni
        y_train_pred = ridge.predict(Phi_train)
        y_test_pred = ridge.predict(Phi_test)

        # Calcolo MSE su train e test
        mse_train = mean_squared_error(t_train, y_train_pred)
        mse_test = mean_squared_error(t_test, y_test_pred)

        mse_results[M][lam] = (mse_train, mse_test)

        # Traccia le curve
        label = f"Œª={lam}  (MSE test={mse_test:.3f})" if lam > 0 else "OLS (Œª=0)"
        ax.plot(X_test, y_test_pred, linewidth=2, label=label)

    # Dati e funzione vera
    ax.scatter(X_train, t_train, color="black", s=40, label="Training data")
    ax.plot(X_test, t_test, "g--", linewidth=2, label="True function")

    ax.set_title(f"Grado polinomiale M = {M}")
    ax.set_ylabel("t")
    ax.legend()

plt.xlabel("x")
plt.suptitle("Effetto combinato di grado M e regolarizzazione Œª (Ridge Regression)", fontsize=14, y=0.93)
plt.tight_layout()
plt.show()

# ------------------------------------------------------------
# Stampa tabellare dei MSE per ogni combinazione (M, Œª)
# ------------------------------------------------------------
print("=== Mean Squared Error (MSE) ===")
for M in degrees:
    print(f"\nGrado polinomiale M = {M}")
    print("Œª\t\tTrain MSE\tTest MSE")
    print("-"*40)
    for lam, (mse_train, mse_test) in mse_results[M].items():
        print(f"{lam:<8g}\t{mse_train:.4f}\t\t{mse_test:.4f}")

"""### üìä Analisi dei risultati ‚Äî Effetto combinato di $M$ e $\lambda$

La tabella del **Mean Squared Error (MSE)** mostra come il comportamento del modello cambi al variare di:

- **$M$** ‚Üí complessit√† del polinomio  
- **$\lambda$** ‚Üí forza della regolarizzazione Ridge  

---

#### üîπ Gradi bassi (M = 3, 5)
- Modelli semplici ‚Üí errori simili su train e test.  
- La regolarizzazione incide poco: i pesi sono gi√† piccoli.  
- Buona generalizzazione, MSE stabile intorno a 0.01.  
üìò Qui la Ridge √® inutile: il modello non √® complesso.

---

#### üîπ Grado medio (M = 9)
- Senza regolarizzazione: **overfitting** (train 0.0024, test 0.0075).  
- Con $\lambda$ piccolo ($10^{-3}$‚Äì$10^{-2}$): test MSE scende a ~0.005.  
- Con $\lambda$ alto: cresce il bias.  
üìò Ridge efficace ‚Äî bilancia complessit√† e stabilit√†.

---

#### üîπ Grado alto (M = 12)
- Senza regolarizzazione: perfetto su train (0.0005), pessimo su test (0.03).  
- Con $\lambda$ piccolo: test MSE migliora (~0.01).  
üìò Leggera regolarizzazione = drastico miglioramento ‚Üí riduce il rumore appreso.

---

#### üîπ Grado molto alto (M = 15)
- $\lambda=0$: test MSE esplode (1.5) ‚Üí segue il rumore.  
- $\lambda=0.001$‚Äì$0.01$: test MSE < 0.1 ‚Üí ottima correzione.  
- $\lambda‚â•0.1$: curva troppo rigida ‚Üí underfitting.  
üìò Con modelli molto flessibili, Ridge √® indispensabile.

---

### üí° Sintesi

| Modello | M | Œª ideale | Effetto |
|----------|---|-----------|----------|
| Semplice | 3‚Äì5 | irrilevante | gi√† stabile |
| Medio | 9 | 0.001‚Äì0.01 | riduce overfitting |
| Complesso | 12‚Äì15 | 0.001‚Äì0.01 | stabilizza e generalizza |

üìà Aumentare $M$ ‚Üí pi√π flessibilit√† ma pi√π varianza.  
üìâ Aumentare $\lambda$ ‚Üí pi√π controllo ma pi√π bias.  
Il miglior punto √® dove il **MSE di test √® minimo**: equilibrio tra adattamento e regolarizzazione.
"""