{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ6oIiA5fSs0"
      },
      "source": [
        "# 🧠 Laboratory 1 — Training and Testing Data  \n",
        "## Machine Learning Course (A.A. 2025–2026)  \n",
        "### Danilo Croce  \n",
        "\n",
        "This first lab is designed to help you **get familiar with the practical environment** used throughout the course.\n",
        "\n",
        "In particular, it will help you:\n",
        "- Understand how to **run Python code interactively** in a Jupyter Notebook  \n",
        "- Gain confidence using **Google Colab** — a cloud-based environment that requires *no local installation*  \n",
        "- Optionally, learn how to **download the notebook** and open it locally with **Anaconda / Jupyter Lab**  \n",
        "- Or even **export it as a `.py` script** to run in any Python environment  \n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Learning objectives\n",
        "In this lab you will:\n",
        "- Load and explore a real dataset (the *Iris* dataset)  \n",
        "- Split data into **training** and **testing** subsets  \n",
        "- Understand why **shuffling** and **stratification** matter in model evaluation  \n",
        "\n",
        "You can either:\n",
        "1. Work directly online on **[Google Colab](https://colab.research.google.com/)**, or  \n",
        "2. Download this notebook and open it locally in **Anaconda / Jupyter Lab**.  \n",
        "\n",
        "All examples use **Python 3** and **scikit-learn**, two core tools you will use throughout the Machine Learning course.\n",
        "\n",
        "💡 *Note: This lab is not about building complex models yet — it’s a warm-up to practice using notebooks and handling data properly.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wTT99q7IgUe"
      },
      "source": [
        "## Packages and Libraries\n",
        "\n",
        "If you are working on **Google Colab**, most of the commonly used packages\n",
        "(e.g. `numpy`, `matplotlib`, `scikit-learn`, `pandas`) are already pre-installed.  \n",
        "So, you can import them directly and start coding right away.\n",
        "\n",
        "If instead you are running this notebook in a **local environment** (e.g. via Anaconda),  \n",
        "some libraries might need to be installed first. For example:\n",
        "\n",
        "```bash\n",
        "conda install scikit-learn\n",
        "```\n",
        "\n",
        "or, with pip:\n",
        "```bash\n",
        "pip install scikit-learn\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsnzgr8JAnle"
      },
      "outputs": [],
      "source": [
        "#Let's get rid of some imports\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFHA0QxBAnlt"
      },
      "source": [
        "## Loading a Simple Dataset (Iris)\n",
        "\n",
        "We will use the **Iris dataset**, a classic and well-known dataset in Machine Learning.\n",
        "\n",
        "It contains **150 samples** of iris flowers, each described by four numerical features:\n",
        "- sepal length  \n",
        "- sepal width  \n",
        "- petal length  \n",
        "- petal width  \n",
        "\n",
        "and a **label** (`y`) that identifies the flower species:\n",
        "- 0 = Setosa  \n",
        "- 1 = Versicolor  \n",
        "- 2 = Virginica  \n",
        "\n",
        "In Python ML code, it’s common to use:\n",
        "- `X` → features (independent variables)  \n",
        "- `y` → target (dependent variable, or label)  \n",
        "\n",
        "👉 Remember: this convention (`X`, `y`) will appear in most ML tutorials, libraries, and research papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGXzpfM6Anlx"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#Iris is available from the sklearn package\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2657
        },
        "id": "dngCjA2pAnl1",
        "outputId": "53e27201-9e33-44a0-bff4-ed90075348fb"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2-jHdnyAnmA"
      },
      "source": [
        "Thinking about how machine learning is normally performed, the idea of a train/test split makes sense.  \n",
        "Real-world systems are trained on existing data, and then must make predictions on **new incoming data** (e.g., from users, sensors, or logs).  \n",
        "\n",
        "We can simulate this during training using a **train/test split**:  \n",
        "- the test data acts as “future data” that the system has never seen before.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why shuffling is necessary\n",
        "In the Iris dataset, the 150 samples are **sorted by class**:  \n",
        "- the first 50 are Setosa,  \n",
        "- then 50 Versicolor,  \n",
        "- then 50 Virginica.  \n",
        "\n",
        "If we split the dataset without shuffling, we risk creating **distorted class distributions**.  \n",
        "For example, a 2/3 vs. 1/3 split would put only Setosa + Versicolor in the training set, and only Virginica in the test set → the model would completely fail.  \n",
        "\n",
        "✅ Therefore, we must **randomly shuffle the dataset before splitting**, unless we are working with time series or sequential data (where order matters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "QkAP8fOUAnmC",
        "outputId": "7c2494c2-5a06-411f-9b6b-8e566b55980a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgu_DSEmAnmJ"
      },
      "source": [
        "## Why Do We Split Data into Training and Testing Sets?\n",
        "\n",
        "When building a model, we want to test whether it can **generalize** — that is, perform well on data it has never seen before.\n",
        "\n",
        "To simulate this, we divide our dataset into two parts:\n",
        "- **Training set:** used to fit the model (the model “learns” from this data)  \n",
        "- **Testing set:** used to evaluate the model’s performance on unseen data  \n",
        "\n",
        "This mimics what happens in the real world:\n",
        "> The model is trained on existing data and must make predictions on future, unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## Shuffling the Dataset\n",
        "The Iris dataset is **sorted by class**:  \n",
        "the first 50 samples are Setosa, then 50 Versicolor, then 50 Virginica.  \n",
        "\n",
        "If we split the dataset in order (without shuffling), we might train on only one or two classes and test on another — leading to completely misleading results.  \n",
        "\n",
        "✅ Therefore, we always **shuffle the dataset before splitting**.  \n",
        "Scikit-learn does this automatically when using `train_test_split`.\n",
        "\n",
        "💡 *Takeaway: If we don’t shuffle properly, the test accuracy we compute is meaningless.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "Tk80smG2AnmK",
        "outputId": "12add226-7683-4ccf-a654-656f3dba0b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels for training and testing data\n",
            "[0 1 2 0 1 2 0 1 2 0 1 1 2 2 1 0 0 2 0 2 0 1 1 0 2 2 1 2 1 0 2 0 2 0 1 2 1\n",
            " 2 1 2 0 2 0 0 1 1 0 1 2 2 0 2 2 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 2 0 2 0 1 1\n",
            " 2]\n",
            "[0 2 1 1 2 0 1 0 2 1 2 2 0 2 0 2 2 1 1 1 2 1 1 0 2 1 0 0 1 2 0 1 1 1 2 0 2\n",
            " 1 2 0 2 0 2 0 1 1 0 1 2 0 0 2 1 1 2 0 1 0 0 1 2 0 2 1 2 2 0 1 2 0 0 2 1 2\n",
            " 2]\n"
          ]
        }
      ],
      "source": [
        "#Import Module\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                                                    train_size=0.5,\n",
        "                                                    test_size=0.5,\n",
        "                                                    random_state=122)\n",
        "print(\"Labels for training and testing data\")\n",
        "print(train_y)\n",
        "print(test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwDDvg7aAnmU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYdmHUZZAnmV"
      },
      "source": [
        "### Stratified Splitting\n",
        "\n",
        "- Especially for relatively small datasets, it’s better to **stratify the split**.  \n",
        "- **Stratification** ensures that the class proportions in the train and test sets match the overall dataset.  \n",
        "- Without stratification, random splits may slightly distort class proportions and bias results.\n",
        "- For large datsets it is not really impactful\n",
        "\n",
        "✅ By passing the labels (`y`) to `train_test_split`, we can enforce stratification.\n",
        "\n",
        "💡 *Takeaway: Always stratify when working on classification tasks, especially if the dataset is imbalanced.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "RfVw4h4iAnmW",
        "outputId": "3d786ba4-5c78-4d4a-9ebb-ff3be50d78d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All: [33.33333333 33.33333333 33.33333333]\n",
            "Training: [36.         33.33333333 30.66666667]\n",
            "Test: [30.66666667 33.33333333 36.        ]\n"
          ]
        }
      ],
      "source": [
        "print('All:', np.bincount(y) / float(len(y)) * 100.0)\n",
        "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
        "print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQdtyRJyAnmZ"
      },
      "source": [
        "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "ppFSthwnAnma",
        "outputId": "62d36299-cbe3-4343-b019-146ad361a763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All: [33.33333333 33.33333333 33.33333333]\n",
            "Training: [33.33333333 33.33333333 33.33333333]\n",
            "Test: [33.33333333 33.33333333 33.33333333]\n"
          ]
        }
      ],
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                                                    train_size=0.5,\n",
        "                                                    test_size=0.5,\n",
        "                                                    random_state=123,\n",
        "                                                    stratify=y)\n",
        "\n",
        "print('All:', np.bincount(y) / float(len(y)) * 100.0)\n",
        "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
        "print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eT-MXrJAnmg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ6oYmLII6Xl"
      },
      "source": [
        "## Running the Notebook Locally\n",
        "\n",
        "So far, we have been running the notebook directly on **Google Colab**.  \n",
        "However, you can also **download this notebook** and run it locally on your machine.\n",
        "\n",
        "Steps:\n",
        "1. Go to `File → Download → Download .ipynb` in Colab.\n",
        "2. Open **Anaconda Navigator** (or use your terminal with Conda).\n",
        "3. Launch **Jupyter Notebook** or **Jupyter Lab** from your environment.\n",
        "4. Open the downloaded `.ipynb` file and run it locally.\n",
        "\n",
        "👉 Remember: in a **local environment**, some packages might not be installed by default.  \n",
        "If you get an `ImportError`, you can install the missing packages"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-lab1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
