# -*- coding: utf-8 -*-
"""ML_Lab1.1_2025_2026_Colab_DatasetSplit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s2feJiR902pIrOEhJrUdedN7eQOC1aqA

# 🧠 Laboratory 1 — Training and Testing Data  
## Machine Learning Course (A.A. 2025–2026)  
### Danilo Croce  

This first lab is designed to help you **get familiar with the practical environment** used throughout the course.

In particular, it will help you:
- Understand how to **run Python code interactively** in a Jupyter Notebook  
- Gain confidence using **Google Colab** — a cloud-based environment that requires *no local installation*  
- Optionally, learn how to **download the notebook** and open it locally with **Anaconda / Jupyter Lab**  
- Or even **export it as a `.py` script** to run in any Python environment  

---

### 🎯 Learning objectives
In this lab you will:
- Load and explore a real dataset (the *Iris* dataset)  
- Split data into **training** and **testing** subsets  
- Understand why **shuffling** and **stratification** matter in model evaluation  

You can either:
1. Work directly online on **[Google Colab](https://colab.research.google.com/)**, or  
2. Download this notebook and open it locally in **Anaconda / Jupyter Lab**.  

All examples use **Python 3** and **scikit-learn**, two core tools you will use throughout the Machine Learning course.

💡 *Note: This lab is not about building complex models yet — it’s a warm-up to practice using notebooks and handling data properly.*

## Packages and Libraries

If you are working on **Google Colab**, most of the commonly used packages
(e.g. `numpy`, `matplotlib`, `scikit-learn`, `pandas`) are already pre-installed.  
So, you can import them directly and start coding right away.

If instead you are running this notebook in a **local environment** (e.g. via Anaconda),  
some libraries might need to be installed first. For example:

```bash
conda install scikit-learn
```

or, with pip:
```bash
pip install scikit-learn
```
"""

# Commented out IPython magic to ensure Python compatibility.
#Let's get rid of some imports
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

"""## Loading a Simple Dataset (Iris)

We will use the **Iris dataset**, a classic and well-known dataset in Machine Learning.

It contains **150 samples** of iris flowers, each described by four numerical features:
- sepal length  
- sepal width  
- petal length  
- petal width  

and a **label** (`y`) that identifies the flower species:
- 0 = Setosa  
- 1 = Versicolor  
- 2 = Virginica  

In Python ML code, it’s common to use:
- `X` → features (independent variables)  
- `y` → target (dependent variable, or label)  

👉 Remember: this convention (`X`, `y`) will appear in most ML tutorials, libraries, and research papers.
"""

from sklearn.datasets import load_iris

#Iris is available from the sklearn package
iris = load_iris()
X, y = iris.data, iris.target

X

"""Thinking about how machine learning is normally performed, the idea of a train/test split makes sense.  
Real-world systems are trained on existing data, and then must make predictions on **new incoming data** (e.g., from users, sensors, or logs).  

We can simulate this during training using a **train/test split**:  
- the test data acts as “future data” that the system has never seen before.  

---

### Why shuffling is necessary
In the Iris dataset, the 150 samples are **sorted by class**:  
- the first 50 are Setosa,  
- then 50 Versicolor,  
- then 50 Virginica.  

If we split the dataset without shuffling, we risk creating **distorted class distributions**.  
For example, a 2/3 vs. 1/3 split would put only Setosa + Versicolor in the training set, and only Virginica in the test set → the model would completely fail.  

✅ Therefore, we must **randomly shuffle the dataset before splitting**, unless we are working with time series or sequential data (where order matters).
"""

y

"""## Why Do We Split Data into Training and Testing Sets?

When building a model, we want to test whether it can **generalize** — that is, perform well on data it has never seen before.

To simulate this, we divide our dataset into two parts:
- **Training set:** used to fit the model (the model “learns” from this data)  
- **Testing set:** used to evaluate the model’s performance on unseen data  

This mimics what happens in the real world:
> The model is trained on existing data and must make predictions on future, unseen data.

---

## Shuffling the Dataset
The Iris dataset is **sorted by class**:  
the first 50 samples are Setosa, then 50 Versicolor, then 50 Virginica.  

If we split the dataset in order (without shuffling), we might train on only one or two classes and test on another — leading to completely misleading results.  

✅ Therefore, we always **shuffle the dataset before splitting**.  
Scikit-learn does this automatically when using `train_test_split`.

💡 *Takeaway: If we don’t shuffle properly, the test accuracy we compute is meaningless.*
"""

#Import Module
from sklearn.model_selection import train_test_split


train_X, test_X, train_y, test_y = train_test_split(X, y,
                                                    train_size=0.5,
                                                    test_size=0.5,
                                                    random_state=122)
print("Labels for training and testing data")
print(train_y)
print(test_y)

"""---

### Stratified Splitting

- Especially for relatively small datasets, it’s better to **stratify the split**.  
- **Stratification** ensures that the class proportions in the train and test sets match the overall dataset.  
- Without stratification, random splits may slightly distort class proportions and bias results.
- For large datsets it is not really impactful

✅ By passing the labels (`y`) to `train_test_split`, we can enforce stratification.

💡 *Takeaway: Always stratify when working on classification tasks, especially if the dataset is imbalanced.*
"""

print('All:', np.bincount(y) / float(len(y)) * 100.0)
print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)
print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)

"""So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:"""

train_X, test_X, train_y, test_y = train_test_split(X, y,
                                                    train_size=0.5,
                                                    test_size=0.5,
                                                    random_state=123,
                                                    stratify=y)

print('All:', np.bincount(y) / float(len(y)) * 100.0)
print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)
print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)

"""---

## Running the Notebook Locally

So far, we have been running the notebook directly on **Google Colab**.  
However, you can also **download this notebook** and run it locally on your machine.

Steps:
1. Go to `File → Download → Download .ipynb` in Colab.
2. Open **Anaconda Navigator** (or use your terminal with Conda).
3. Launch **Jupyter Notebook** or **Jupyter Lab** from your environment.
4. Open the downloaded `.ipynb` file and run it locally.

👉 Remember: in a **local environment**, some packages might not be installed by default.  
If you get an `ImportError`, you can install the missing packages
"""